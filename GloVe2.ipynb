{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD295iy42cg1",
        "outputId": "70f21ef6-3ed0-4017-fd36-773f454a0b8b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM,Bidirectional, GlobalMaxPooling1D,Dropout,Flatten,Conv1D,MaxPooling1D\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MczVfr6I2o7-",
        "outputId": "fbe57903-f91e-4d90-e515-cae413994023"
      },
      "source": [
        "data = pd.read_csv('version-4.csv',index_col=0)\n",
        "data.reset_index(drop=True,inplace = True)\n",
        "\n",
        "for i,j in enumerate(data['labels']):\n",
        "  try:\n",
        "    j = int(j)\n",
        "  except ValueError as e:\n",
        "    print(f'error on {i} line')\n",
        "data.drop(labels=[2478],inplace= True)\n",
        "data['labels'].astype(int);\n",
        "data.dropna(inplace = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error on 2478 line\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqRK7SHV28zy"
      },
      "source": [
        "cleaned_text = []\n",
        "for text in data['texts']:\n",
        "  text = \" \".join(word for word in text.split() if not word.isdigit())\n",
        "  cleaned_text.append(text)\n",
        "data['cleaned_text'] = cleaned_text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mxx66Xz3AIJ"
      },
      "source": [
        "vocab =  {}\n",
        "for text in data['cleaned_text']:\n",
        "  sen = text.split()\n",
        "  for word in sen:\n",
        "    try:\n",
        "      vocab[word] += 1\n",
        "    except KeyError:\n",
        "      vocab[word] = 1\n",
        "vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ5K0hs53DjZ"
      },
      "source": [
        "rare_words = []\n",
        "for key,value in vocab.items():\n",
        "  if value<=10:\n",
        "    rare_words.append(key)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvn8Y-oD3GcO"
      },
      "source": [
        "stopwords_en = set(stopwords.words('english'))\n",
        "cleaner_text = []\n",
        "for text in data['cleaned_text']:\n",
        "  text = \" \".join([word for word in text.split() if len(word)>2 and word not in rare_words])\n",
        "  cleaner_text.append(text)\n",
        "data['final_text'] = cleaner_text"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMzTqM5j3KII"
      },
      "source": [
        "vocab =  {}\n",
        "for text in data['final_text']:\n",
        "  sen = text.split()\n",
        "  for word in sen:\n",
        "    try:\n",
        "      vocab[word] += 1\n",
        "    except KeyError:\n",
        "      vocab[word] = 1\n",
        "vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMZgFbC33QSQ"
      },
      "source": [
        "vocab_list = list(vocab.items())\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW1cvb0d3S3z"
      },
      "source": [
        "x = data['final_text'].values\n",
        "y = data['labels'].values\n",
        "X_train,X_test,y_train, y_test = train_test_split(x,y, test_size = 0.2, shuffle = True)\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiCTig0y3j9u",
        "outputId": "aee1ddb3-a83e-44b3-96d8-11ee9621e2a0"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  try:\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "  except ValueError:\n",
        "    pass\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 26227 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3xl2Fz33WSb"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_train = sequence.pad_sequences(X_train,maxlen=200)\n",
        "X_test = sequence.pad_sequences(X_test,maxlen=200)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKPmTVn3Yvs"
      },
      "source": [
        "tokens = len(tokenizer.word_index) + 2\n",
        "embedding_matrix = np.zeros((tokens, 200))\n",
        "count = 0\n",
        "unknown = []\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    try:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    except ValueError:\n",
        "      unknown.append(word)\n",
        "      count += 1\n",
        "  else:\n",
        "    unknown.append(word)\n",
        "    count += 1"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIUo5bUs3b5o",
        "outputId": "f379d39f-ce7b-4c79-ed8e-3971825a1bcc"
      },
      "source": [
        "print(1-(count/vocab_size))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46176411760784053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7zckA9U3sKp",
        "outputId": "29159144-ccb8-48f4-8555-436c7ef7a058"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(tokens,200,weights = [embedding_matrix],input_length = embedding_matrix.shape[1]))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 200)          2579800   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               120400    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 2,700,301\n",
            "Trainable params: 2,700,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6S5NSD63xdz",
        "outputId": "66d370a4-fd73-4a10-950c-3899b3a90336"
      },
      "source": [
        "model.fit(X_train,y_train,epochs=30)\n",
        "loss,accuracy = model.evaluate(X_train,y_train)\n",
        "print(f'acc: {accuracy}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "323/323 [==============================] - 14s 40ms/step - loss: 0.4307 - accuracy: 0.7924\n",
            "Epoch 2/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.2452 - accuracy: 0.9002\n",
            "Epoch 3/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.1789 - accuracy: 0.9344\n",
            "Epoch 4/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.1249 - accuracy: 0.9576\n",
            "Epoch 5/30\n",
            "323/323 [==============================] - 13s 41ms/step - loss: 0.1050 - accuracy: 0.9625\n",
            "Epoch 6/30\n",
            "323/323 [==============================] - 13s 41ms/step - loss: 0.1396 - accuracy: 0.9511\n",
            "Epoch 7/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0982 - accuracy: 0.9632\n",
            "Epoch 8/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0810 - accuracy: 0.9711\n",
            "Epoch 9/30\n",
            "323/323 [==============================] - 12s 38ms/step - loss: 0.0687 - accuracy: 0.9764\n",
            "Epoch 10/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0679 - accuracy: 0.9740\n",
            "Epoch 11/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0602 - accuracy: 0.9781\n",
            "Epoch 12/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.0526 - accuracy: 0.9795\n",
            "Epoch 13/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0445 - accuracy: 0.9825\n",
            "Epoch 14/30\n",
            "323/323 [==============================] - 12s 38ms/step - loss: 0.0418 - accuracy: 0.9846\n",
            "Epoch 15/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0373 - accuracy: 0.9855\n",
            "Epoch 16/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0317 - accuracy: 0.9877\n",
            "Epoch 17/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0309 - accuracy: 0.9884\n",
            "Epoch 18/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0263 - accuracy: 0.9903\n",
            "Epoch 19/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.0247 - accuracy: 0.9911\n",
            "Epoch 20/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0276 - accuracy: 0.9903\n",
            "Epoch 21/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0179 - accuracy: 0.9932\n",
            "Epoch 22/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0160 - accuracy: 0.9942\n",
            "Epoch 23/30\n",
            "323/323 [==============================] - 12s 39ms/step - loss: 0.0139 - accuracy: 0.9951\n",
            "Epoch 24/30\n",
            "323/323 [==============================] - 12s 38ms/step - loss: 0.0127 - accuracy: 0.9958\n",
            "Epoch 25/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0137 - accuracy: 0.9949\n",
            "Epoch 26/30\n",
            "323/323 [==============================] - 13s 40ms/step - loss: 0.0153 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0134 - accuracy: 0.9953\n",
            "Epoch 28/30\n",
            "323/323 [==============================] - 12s 38ms/step - loss: 0.0103 - accuracy: 0.9971\n",
            "Epoch 29/30\n",
            "323/323 [==============================] - 12s 38ms/step - loss: 0.0078 - accuracy: 0.9974\n",
            "Epoch 30/30\n",
            "323/323 [==============================] - 13s 39ms/step - loss: 0.0089 - accuracy: 0.9967\n",
            "323/323 [==============================] - 2s 6ms/step - loss: 0.0074 - accuracy: 0.9978\n",
            "acc: 0.9977747797966003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4ypG0k3zuU"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "predictions = np.round(predictions)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Umu3oD3B2XQ",
        "outputId": "5a29eebd-3893-4c57-ea30-e0c597825b05"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "score = accuracy_score(y_test,predictions)\n",
        "cm = confusion_matrix(y_test,predictions)\n",
        "print(\"score: {} cm: {}\".format(score,cm))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 0.8750483558994198 cm: [[1191  179]\n",
            " [ 144 1071]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElMzrtHerHLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}